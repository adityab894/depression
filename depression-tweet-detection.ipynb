{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2186313,"sourceType":"datasetVersion","datasetId":1312443},{"sourceId":7408944,"sourceType":"datasetVersion","datasetId":4309154},{"sourceId":6068,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":4689}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries, loading and transforming data","metadata":{"_uuid":"2c60f615-655e-410a-bba7-4837d24d23d0","_cell_guid":"5500e42a-f1cf-4903-b34c-eb7d197c7592","trusted":true}},{"cell_type":"code","source":"!pip install -U -q mlflow datasets>=2.14.5 nlp 2>/dev/null","metadata":{"_uuid":"44bb1450-692d-4c47-9ff0-dd4c46005076","_cell_guid":"5faf0a7d-a4b2-4d50-9cf3-c46675f4aa87","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:35:22.482142Z","iopub.execute_input":"2024-01-16T17:35:22.482502Z","iopub.status.idle":"2024-01-16T17:35:41.082944Z","shell.execute_reply.started":"2024-01-16T17:35:22.482470Z","shell.execute_reply":"2024-01-16T17:35:41.081645Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install tweepy","metadata":{"_uuid":"4ada680b-a71f-4096-8783-951c357baffc","_cell_guid":"936bb276-5519-4cba-ad20-2b0d4fb2c6b0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:35:41.084836Z","iopub.execute_input":"2024-01-16T17:35:41.085164Z","iopub.status.idle":"2024-01-16T17:35:53.005288Z","shell.execute_reply.started":"2024-01-16T17:35:41.085129Z","shell.execute_reply":"2024-01-16T17:35:53.004357Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tweepy\n  Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from tweepy) (3.2.2)\nRequirement already satisfied: requests<3,>=2.27.0 in /opt/conda/lib/python3.10/site-packages (from tweepy) (2.31.0)\nRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from tweepy) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (2023.7.22)\nInstalling collected packages: tweepy\nSuccessfully installed tweepy-4.14.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # For data manipulation and analysis\nimport gc  # For garbage collection to manage memory\nimport re  # For regular expressions\nimport numpy as np  # For numerical operations and arrays\n\nimport warnings  # For handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warning messages\n# import tweepy\nimport torch  # PyTorch library for deep learning\nfrom transformers import AutoModel, AutoTokenizer  # Transformers library for natural language processing\nfrom transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\npipeline, Trainer, TrainingArguments, DataCollatorWithPadding  # Transformers components for text processing\nfrom transformers import AutoModelForSequenceClassification  # Transformer model for sequence classification\n\nfrom nlp import Dataset  # Import custom 'Dataset' class for natural language processing tasks\nfrom imblearn.over_sampling import RandomOverSampler  # For oversampling to handle class imbalance\nimport datasets  # Import datasets library\nfrom datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import pipeline  # Transformers library for pipelines\nfrom bs4 import BeautifulSoup  # For parsing HTML content\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt  # For data visualization\nimport itertools  # For working with iterators\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\nfrom datasets import load_metric  # Import load_metric function to load evaluation metrics\n\nfrom tqdm import tqdm  # For displaying progress bars\ntqdm.pandas()  # Enable progress bars for pandas operations","metadata":{"_uuid":"8f68ed44-04b3-4440-b86d-2e5cb31cafec","_cell_guid":"7bde673d-3ef3-4ec2-adfc-2559a1319de2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:35:53.006687Z","iopub.execute_input":"2024-01-16T17:35:53.006970Z","iopub.status.idle":"2024-01-16T17:36:11.774882Z","shell.execute_reply.started":"2024-01-16T17:35:53.006938Z","shell.execute_reply":"2024-01-16T17:36:11.774050Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Fraction of the dataset used for training, the rest will be used for validation\ntrain_fraction = 0.8\n\n# Number of training epochs\nnum_train_epochs = 20\n\n# Learning rate\nlearning_rate = 5e-7\n\n# Batch size for training\ntrain_batch_size = 8\n\n# Batch size for validation\neval_batch_size = 64\n\n# Number of warm-up steps during training\nwarmup_steps = 50\n\n# Weight decay to control regularization during training\nweight_decay = 0.02\n\n# Pre-trained BERT model to be used\nBERT_MODEL = \"distilbert-base-cased\"\n\n# Directory where the model output will be saved\noutput_dir = \"depressed-tweet-detection-distilbert\"","metadata":{"_uuid":"aed10adf-259e-4fa5-a806-18b0c3b8f468","_cell_guid":"a4260684-cb07-4ff3-aec6-70728cd0c7bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:11.776958Z","iopub.execute_input":"2024-01-16T17:36:11.777661Z","iopub.status.idle":"2024-01-16T17:36:11.783192Z","shell.execute_reply.started":"2024-01-16T17:36:11.777630Z","shell.execute_reply":"2024-01-16T17:36:11.782156Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%time\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(\"/kaggle/input/sentimental-analysis-for-tweets/sentiment_tweets3.csv\", encoding='latin-1', index_col='Index')\n\nitem0 = df.shape[0]  # Store the initial number of items in the DataFrame\ndf = df.drop_duplicates()  # Remove duplicate rows from the DataFrame\nitem1 = df.shape[0]  # Store the number of items in the DataFrame after removing duplicates\nprint(f\"There are {item0-item1} duplicates found in the dataset\")  # Print the number of duplicates removed\n\ndf = df.rename(columns={'label (depression result)': 'label', 'message to examine': 'title'}) # Rename the columns to standard ones\n\n# update https://stackoverflow.com/a/54206513\nURL_REGEX = re.compile(\"http[s]?://\\S+\")\nMENTION_REGEX = re.compile('@\\w+')\n\ndef clean_tweet(tweet):\n    # remove mentions, the pound sign, and replace urls with URL token\n    tweet = re.sub(URL_REGEX, 'url', tweet)  # replace urls with url. Assumes that the mention of a url is significant\n    tweet = re.sub(MENTION_REGEX, '', tweet)  # remove mentions entirely\n    tweet = tweet.replace('#', '')  # remove pound signs\n    \n    return tweet.strip()\n\ndf['title'] = df['title'].apply(clean_tweet)\n\ndef change_label(x):\n    if x:\n        return 'Depressed'\n    else:\n        return 'Not Depressed'\ndf['label'] = df['label'].apply(change_label)\n\n\n\ndf = df[['label', 'title']]  # Select only the 'label' and 'title' columns\ndf = df[~df['title'].isnull()]  # Remove rows where 'title' is null\ndf = df[~df['label'].isnull()]  # Remove rows where 'label' is null\n\nprint(df.shape)  # Print the shape of the DataFrame after data preprocessing\ndf.sample(5).T  # Display a random sample of 5 rows from the DataFrame","metadata":{"_uuid":"85344af3-a709-4481-b07a-3585bbf34ba0","_cell_guid":"0328883a-33b0-41af-8a52-66cccffe607e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:11.784388Z","iopub.execute_input":"2024-01-16T17:36:11.784718Z","iopub.status.idle":"2024-01-16T17:36:11.951364Z","shell.execute_reply.started":"2024-01-16T17:36:11.784679Z","shell.execute_reply":"2024-01-16T17:36:11.950396Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"There are 31 duplicates found in the dataset\n(10283, 2)\nCPU times: user 82.2 ms, sys: 9.11 ms, total: 91.3 ms\nWall time: 131 ms\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index                   313245  \\\nlabel            Not Depressed   \ntitle  so when can we see it??   \n\nIndex                                             441006             218079  \\\nlabel                                      Not Depressed      Not Depressed   \ntitle  SOrry i was having issues with my computer  Te...  the sun loves me.   \n\nIndex                                             49376   \\\nlabel                                      Not Depressed   \ntitle  i start my step up internship on may 4th!! see...   \n\nIndex                                             600211  \nlabel                                      Not Depressed  \ntitle  got my AP with Blink on the coverr  so excited...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>313245</th>\n      <th>441006</th>\n      <th>218079</th>\n      <th>49376</th>\n      <th>600211</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>label</th>\n      <td>Not Depressed</td>\n      <td>Not Depressed</td>\n      <td>Not Depressed</td>\n      <td>Not Depressed</td>\n      <td>Not Depressed</td>\n    </tr>\n    <tr>\n      <th>title</th>\n      <td>so when can we see it??</td>\n      <td>SOrry i was having issues with my computer  Te...</td>\n      <td>the sun loves me.</td>\n      <td>i start my step up internship on may 4th!! see...</td>\n      <td>got my AP with Blink on the coverr  so excited...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Import the necessary library to compute class weights.\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Identify the unique classes in the training data.\nclasses = np.unique(df[['label']])\n\nprint(classes)\n\n# Calculate class weights using the 'balanced' option, which automatically adjusts for class imbalance.\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=df['label'])\n\n# Create a dictionary mapping each class to its respective class weight.\nclass_weights = dict(zip(classes, weights))\n\n# Print the computed class weights to the console.\nprint(class_weights)","metadata":{"_uuid":"82f17d60-ac1e-432c-a44f-b72b913cb786","_cell_guid":"a86b8bea-4438-4548-8eff-e05fe2b98188","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:11.952696Z","iopub.execute_input":"2024-01-16T17:36:11.953036Z","iopub.status.idle":"2024-01-16T17:36:11.973199Z","shell.execute_reply.started":"2024-01-16T17:36:11.953007Z","shell.execute_reply":"2024-01-16T17:36:11.972255Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['Depressed' 'Not Depressed']\n{'Depressed': 2.24814167031045, 'Not Depressed': 0.6430090045022512}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a list of unique labels\nlabels_list = list(df['label'].unique())\n\n# Initialize empty dictionaries to map labels to IDs and vice versa\nlabel2id, id2label = dict(), dict()\n\n# Iterate over the unique labels and assign each label an ID, and vice versa\nfor i, label in enumerate(labels_list):\n    label2id[label] = i  # Map the label to its corresponding ID\n    id2label[i] = label  # Map the ID to its corresponding label\n\n# Print the resulting dictionaries for reference\nprint(\"Mapping of IDs to Labels:\", id2label, '\\n')\nprint(\"Mapping of Labels to IDs:\", label2id)","metadata":{"_uuid":"662828c9-9c58-497e-ab64-fe09c1834bd0","_cell_guid":"232c62d7-cda0-4298-b01c-7f4bfc9cd21e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:11.974830Z","iopub.execute_input":"2024-01-16T17:36:11.975282Z","iopub.status.idle":"2024-01-16T17:36:11.983548Z","shell.execute_reply.started":"2024-01-16T17:36:11.975243Z","shell.execute_reply":"2024-01-16T17:36:11.982732Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Mapping of IDs to Labels: {0: 'Not Depressed', 1: 'Depressed'} \n\nMapping of Labels to IDs: {'Not Depressed': 0, 'Depressed': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"ordered_weigths = [class_weights[x] for x in id2label.values()]\nordered_weigths","metadata":{"_uuid":"c165f722-3bab-4b0d-bb3f-c6506e5d4c66","_cell_guid":"9ac33006-08ca-4ccb-9c7f-4284aec9923c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:11.984978Z","iopub.execute_input":"2024-01-16T17:36:11.985332Z","iopub.status.idle":"2024-01-16T17:36:11.995437Z","shell.execute_reply.started":"2024-01-16T17:36:11.985297Z","shell.execute_reply":"2024-01-16T17:36:11.994243Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[0.6430090045022512, 2.24814167031045]"},"metadata":{}}]},{"cell_type":"code","source":"# Create a dataset from the Pandas DataFrame 'df'\ndataset = Dataset.from_pandas(df)","metadata":{"_uuid":"1c972c59-1df8-4240-85eb-9e46ce8e65d6","_cell_guid":"7dc19fc2-61d0-4289-9472-d3a946c50d55","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:11.996472Z","iopub.execute_input":"2024-01-16T17:36:11.996829Z","iopub.status.idle":"2024-01-16T17:36:12.031591Z","shell.execute_reply.started":"2024-01-16T17:36:11.996806Z","shell.execute_reply":"2024-01-16T17:36:12.030838Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Creating classlabels to match labels to IDs\nClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n\n# Mapping labels to IDs\ndef map_label2id(example):\n    example['label'] = ClassLabels.str2int(example['label'])\n    return example\n\ndataset = dataset.map(map_label2id, batched=True)\n\n# Casting label column to ClassLabel Object\ndataset = dataset.cast_column('label', ClassLabels)\n\n# Splitting the dataset into training and testing sets using the predefined train/test split ratio.\ndataset = dataset.train_test_split(test_size=1-train_fraction, shuffle=True, stratify_by_column=\"label\")\n\n# Extracting the training data from the split dataset.\ndf_train = dataset['train']\n\n# Extracting the testing data from the split dataset.\ndf_test = dataset['test']","metadata":{"_uuid":"1b69b759-89d2-4937-bc06-f5402df1f1ea","_cell_guid":"5b21e0fc-2223-4012-a763-2dabe574ceb3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:12.034914Z","iopub.execute_input":"2024-01-16T17:36:12.035191Z","iopub.status.idle":"2024-01-16T17:36:12.145687Z","shell.execute_reply.started":"2024-01-16T17:36:12.035166Z","shell.execute_reply":"2024-01-16T17:36:12.144887Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10283 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a4810c6cbfb46ae810ed570298115ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/10283 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9087c3225ffa41078ee104963431f129"}},"metadata":{}}]},{"cell_type":"code","source":"# Deleting the DataFrame 'df'\ndel df\n\n# Performing garbage collection to free up memory\ngc.collect()","metadata":{"_uuid":"4da11cbf-365b-47ca-b8f0-86064dce195e","_cell_guid":"90227f03-9ce3-4c9d-9c00-de2b70ff21cc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:12.146700Z","iopub.execute_input":"2024-01-16T17:36:12.146962Z","iopub.status.idle":"2024-01-16T17:36:12.441047Z","shell.execute_reply.started":"2024-01-16T17:36:12.146938Z","shell.execute_reply":"2024-01-16T17:36:12.440147Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"220"},"metadata":{}}]},{"cell_type":"code","source":"# Create a tokenizer instance for the specified BERT model.\ntokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=True, low_cpu_mem_usage=False)","metadata":{"_uuid":"eb137969-0287-4128-9091-7b7967c2ab8e","_cell_guid":"87f036b2-aa5f-485c-8338-2024d8ce0140","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:12.442483Z","iopub.execute_input":"2024-01-16T17:36:12.442900Z","iopub.status.idle":"2024-01-16T17:36:13.537863Z","shell.execute_reply.started":"2024-01-16T17:36:12.442851Z","shell.execute_reply":"2024-01-16T17:36:13.536955Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3650d5a5e5c94b08a282f7b03d4bd98f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20cc458e71ca49e29763121b25e6bb71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72a3a72730b4639891e13b47d2cea92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94dd0250b75b4cc7ab4bb5b7a15a9097"}},"metadata":{}}]},{"cell_type":"code","source":"# Importantly, this is a simple function for preprocessing data before training a natural language processing model.\n# It takes a dataset of examples as input.\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"title\"], truncation=True)\n\n# The code below applies the preprocess_function to two dataframes, df_train and df_test.\n\ndf_train = df_train.map(preprocess_function, batched=True)\ndf_test = df_test.map(preprocess_function, batched=True)","metadata":{"_uuid":"e213eba0-bedb-4590-bb57-a16a97ac381c","_cell_guid":"2fbaf587-9c76-423c-a285-ef43152bd9d0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:13.539136Z","iopub.execute_input":"2024-01-16T17:36:13.539513Z","iopub.status.idle":"2024-01-16T17:36:14.519446Z","shell.execute_reply.started":"2024-01-16T17:36:13.539477Z","shell.execute_reply":"2024-01-16T17:36:14.518608Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8226 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d870dd5020f4f2c8abf447bdcffbbc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2057 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf06e03c746144c4aacb4adfab5c6942"}},"metadata":{}}]},{"cell_type":"code","source":"# Remove the 'title' column from the training dataset.\ndf_train = df_train.remove_columns(['title'])\n\n# Remove the 'title' column from the testing dataset.\ndf_test = df_test.remove_columns(['title'])","metadata":{"_uuid":"70ec35bd-929b-4654-be3e-3bdc98c87103","_cell_guid":"7cce836c-4028-4b18-aedd-d7d0619a264c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:14.520695Z","iopub.execute_input":"2024-01-16T17:36:14.520983Z","iopub.status.idle":"2024-01-16T17:36:14.530090Z","shell.execute_reply.started":"2024-01-16T17:36:14.520956Z","shell.execute_reply":"2024-01-16T17:36:14.529250Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"_uuid":"d5f94f5c-1902-4dbf-a475-6be8a02aa1ed","_cell_guid":"2fbfde45-2498-4b41-a206-2c0142ab57bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:14.531338Z","iopub.execute_input":"2024-01-16T17:36:14.531745Z","iopub.status.idle":"2024-01-16T17:36:14.540688Z","shell.execute_reply.started":"2024-01-16T17:36:14.531709Z","shell.execute_reply":"2024-01-16T17:36:14.539837Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['label', 'Index', 'input_ids', 'attention_mask'],\n    num_rows: 8226\n})"},"metadata":{}}]},{"cell_type":"code","source":"df_test","metadata":{"_uuid":"c56fa26a-8b3e-4a55-bfa2-963748b9e718","_cell_guid":"66083bbe-20d4-497b-a784-490d6eb35cab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:14.541965Z","iopub.execute_input":"2024-01-16T17:36:14.542300Z","iopub.status.idle":"2024-01-16T17:36:14.552128Z","shell.execute_reply.started":"2024-01-16T17:36:14.542266Z","shell.execute_reply":"2024-01-16T17:36:14.551224Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['label', 'Index', 'input_ids', 'attention_mask'],\n    num_rows: 2057\n})"},"metadata":{}}]},{"cell_type":"code","source":"# DataCollatorWithPadding creates batch of data. It also dynamically pads text to the \n#  It's possible to pad your text in the tokenizer function with padding=True, dynamic padding is more efficient.\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"_uuid":"0aabba9b-3b94-4341-8dfa-ba7c7381b0dc","_cell_guid":"fd827c2c-9eb1-40ad-ab4a-200b321d016a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:14.553461Z","iopub.execute_input":"2024-01-16T17:36:14.553913Z","iopub.status.idle":"2024-01-16T17:36:14.560978Z","shell.execute_reply.started":"2024-01-16T17:36:14.553878Z","shell.execute_reply":"2024-01-16T17:36:14.560186Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Retrieve the 'input_ids' from the first row of the DataFrame 'df_train'\ntokenizer.decode(df_train[0]['input_ids'])","metadata":{"_uuid":"ccae5ea4-ef96-418b-b247-ea5d66bc1581","_cell_guid":"1a8d3667-16ac-4f67-8bb7-0f6fb1aa5fba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:14.562109Z","iopub.execute_input":"2024-01-16T17:36:14.562408Z","iopub.status.idle":"2024-01-16T17:36:14.577833Z","shell.execute_reply.started":"2024-01-16T17:36:14.562370Z","shell.execute_reply":"2024-01-16T17:36:14.576967Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'[CLS] is embarking on a five day weekend! Beaches, ferries, and fun! Life is good! [SEP]'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading and training model","metadata":{"_uuid":"8cfbf6c3-07d9-4d8e-8f2d-15e1abbade42","_cell_guid":"5a5a5ea2-f2f7-4947-8e59-b6aeee97b295","trusted":true}},{"cell_type":"code","source":"# Load a pre-trained BERT-based model for sequence classification.\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    BERT_MODEL, num_labels=len(labels_list),\n    output_attentions=False,  # Set to False: Model will not return attention weights.\n    output_hidden_states=False  # Set to False: Model will not return all hidden-states.\n)\n\n# Configure the mapping of class labels to their corresponding indices for later reference.\nmodel.config.id2label = id2label  # Mapping from label indices to class labels.\nmodel.config.label2id = label2id  # Mapping from class labels to label indices.\n\n# Calculate and print the number of trainable parameters in millions for the model.\nprint(model.num_parameters(only_trainable=True) / 1e6)","metadata":{"_uuid":"33d2f28c-4259-4fe1-baed-49b238c4b6af","_cell_guid":"c0c57342-6107-43c9-99fa-78e8dbd1b5ad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:14.578957Z","iopub.execute_input":"2024-01-16T17:36:14.579280Z","iopub.status.idle":"2024-01-16T17:36:16.564055Z","shell.execute_reply.started":"2024-01-16T17:36:14.579245Z","shell.execute_reply":"2024-01-16T17:36:16.563051Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61ce353bf8e49e0b31b2cb3ad12a898"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"65.783042\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the 'load_metric' function from the Hugging Face datasets library to load a metric.\nmetric = load_metric(\"accuracy\")\n\n# Define a custom 'compute_metrics' function that will be used for evaluating model performance.\n# This function takes 'eval_pred' as input, which is a tuple containing predicted logits and true labels.\ndef compute_metrics(eval_pred):\n    # Unpack the 'eval_pred' tuple into 'logits' (predicted logits) and 'labels' (true labels).\n    logits, labels = eval_pred\n    \n    # Calculate the model's predictions by selecting the class with the highest logit value.\n    predictions = np.argmax(logits, axis=-1)\n    \n    # Use the imported metric to compute the accuracy of the model's predictions.\n    accuracy = metric.compute(predictions=predictions, references=labels)\n    \n    # Return the computed accuracy as the evaluation metric.\n    return accuracy","metadata":{"_uuid":"59133a13-d9a1-4be5-b9da-fd40875fdf2f","_cell_guid":"60242e59-89fc-4ba2-887c-39608b808320","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:16.565238Z","iopub.execute_input":"2024-01-16T17:36:16.565532Z","iopub.status.idle":"2024-01-16T17:36:17.141198Z","shell.execute_reply.started":"2024-01-16T17:36:16.565506Z","shell.execute_reply":"2024-01-16T17:36:17.140460Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b13cda243f4976b1c238408b7674c9"}},"metadata":{}}]},{"cell_type":"code","source":"class WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # compute custom loss (suppose one has labels with different weights)\n        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor(ordered_weigths, device=model.device).float())\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"_uuid":"4ce6ab4c-6b53-4edb-b519-cb669b90b41a","_cell_guid":"eb17b85b-c472-45a2-8a02-22b98d1465a4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:17.142277Z","iopub.execute_input":"2024-01-16T17:36:17.142591Z","iopub.status.idle":"2024-01-16T17:36:17.148940Z","shell.execute_reply.started":"2024-01-16T17:36:17.142544Z","shell.execute_reply":"2024-01-16T17:36:17.147852Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Create TrainingArguments to configure the training process\ntraining_args = TrainingArguments(\n    output_dir=output_dir,  # Directory to save the model checkpoints and logs\n    logging_dir='./logs',  # Directory to store training logs\n    num_train_epochs=num_train_epochs,  # Number of training epochs\n    per_device_train_batch_size=train_batch_size,  # Batch size for training data\n    per_device_eval_batch_size=eval_batch_size,  # Batch size for evaluation data\n    logging_strategy='steps',  # Logging frequency during training (steps or epoch)\n    logging_first_step=True,  # Log the first training step\n    load_best_model_at_end=True,  # Load the best model at the end of training\n    logging_steps=1,  # Log every training step (useful for debugging)\n    learning_rate=learning_rate, # Set the learning rate for the optimizer.\n    evaluation_strategy='epoch',  # Evaluation frequency (epoch or steps)\n    warmup_steps=warmup_steps,  # Number of warmup steps for the learning rate\n    weight_decay=weight_decay,  # Weight decay for regularization\n    eval_steps=1,  # Evaluate every training step (useful for debugging)\n    save_strategy='epoch',  # Save model checkpoints every epoch\n    save_total_limit=1,  # Limit the number of saved checkpoints to save space\n    report_to=\"mlflow\",  # Log training metrics to MLflow\n)\n\n# Define the trainer:\n# Instantiate the trainer class and configure its settings\ntrainer = WeightedTrainer(\n    model=model,  # The pretrained or custom model to be trained\n    args=training_args,  # TrainingArguments for configuring training\n    compute_metrics=compute_metrics,  # Function for computing evaluation metrics\n    train_dataset=df_train,  # Training dataset\n    eval_dataset=df_test,  # Evaluation dataset\n    data_collator=data_collator  # Data collator for batching and preprocessing\n)","metadata":{"_uuid":"bf402f83-8567-4c1c-a5bc-6ea95f5c7199","_cell_guid":"0eaf434d-7e33-424d-9876-725ea27cb759","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:17.150212Z","iopub.execute_input":"2024-01-16T17:36:17.150549Z","iopub.status.idle":"2024-01-16T17:36:17.977914Z","shell.execute_reply.started":"2024-01-16T17:36:17.150516Z","shell.execute_reply":"2024-01-16T17:36:17.976873Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Get initial metrics\ntrainer.evaluate()","metadata":{"_uuid":"9ed680c2-4781-466d-9a5b-897185e3f584","_cell_guid":"9548615b-0641-4516-9f80-946e2692db52","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:17.979267Z","iopub.execute_input":"2024-01-16T17:36:17.979879Z","iopub.status.idle":"2024-01-16T17:36:21.634733Z","shell.execute_reply.started":"2024-01-16T17:36:17.979850Z","shell.execute_reply":"2024-01-16T17:36:21.633643Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [33/33 00:49]\n    </div>\n    "},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.6991604566574097,\n 'eval_accuracy': 0.22216820612542537,\n 'eval_runtime': 3.5517,\n 'eval_samples_per_second': 579.154,\n 'eval_steps_per_second': 9.291}"},"metadata":{}}]},{"cell_type":"code","source":"# Start training the model\ntrainer.train()","metadata":{"_uuid":"6889d1c2-5a4a-4577-9959-9f1abebaebfe","_cell_guid":"0ecbf0d3-f45c-4f8b-9d56-9164f547759d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:36:21.636100Z","iopub.execute_input":"2024-01-16T17:36:21.636392Z","iopub.status.idle":"2024-01-16T17:52:18.852219Z","shell.execute_reply.started":"2024-01-16T17:36:21.636367Z","shell.execute_reply":"2024-01-16T17:52:18.851278Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20580' max='20580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20580/20580 15:56, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.280400</td>\n      <td>0.256196</td>\n      <td>0.969373</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.014300</td>\n      <td>0.045942</td>\n      <td>0.993680</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.004500</td>\n      <td>0.023260</td>\n      <td>0.997083</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.002100</td>\n      <td>0.016910</td>\n      <td>0.997569</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.001800</td>\n      <td>0.015574</td>\n      <td>0.997569</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000800</td>\n      <td>0.015796</td>\n      <td>0.997569</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000700</td>\n      <td>0.017737</td>\n      <td>0.997083</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000700</td>\n      <td>0.016683</td>\n      <td>0.997083</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000400</td>\n      <td>0.012521</td>\n      <td>0.998055</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000300</td>\n      <td>0.023740</td>\n      <td>0.998055</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.000700</td>\n      <td>0.017239</td>\n      <td>0.998055</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.000500</td>\n      <td>0.012459</td>\n      <td>0.998055</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.000400</td>\n      <td>0.013513</td>\n      <td>0.998055</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.001000</td>\n      <td>0.014702</td>\n      <td>0.998055</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.000400</td>\n      <td>0.010882</td>\n      <td>0.998542</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.000700</td>\n      <td>0.011531</td>\n      <td>0.998055</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.000400</td>\n      <td>0.011052</td>\n      <td>0.998542</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.000300</td>\n      <td>0.010936</td>\n      <td>0.998542</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.000400</td>\n      <td>0.011442</td>\n      <td>0.998542</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.000100</td>\n      <td>0.010925</td>\n      <td>0.998055</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20580, training_loss=0.05439883889622376, metrics={'train_runtime': 956.6965, 'train_samples_per_second': 171.967, 'train_steps_per_second': 21.512, 'total_flos': 2145263985565632.0, 'train_loss': 0.05439883889622376, 'epoch': 20.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Final model evaluation\ntrainer.evaluate()","metadata":{"_uuid":"d3c869e3-10e3-4c75-9865-91876c945a71","_cell_guid":"d559c85a-9f13-4500-b577-11eaf301e65b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:18.853672Z","iopub.execute_input":"2024-01-16T17:52:18.854382Z","iopub.status.idle":"2024-01-16T17:52:21.809294Z","shell.execute_reply.started":"2024-01-16T17:52:18.854345Z","shell.execute_reply":"2024-01-16T17:52:21.808387Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.010881882160902023,\n 'eval_accuracy': 0.9985415653864852,\n 'eval_runtime': 2.9298,\n 'eval_samples_per_second': 702.097,\n 'eval_steps_per_second': 11.264,\n 'epoch': 20.0}"},"metadata":{}}]},{"cell_type":"code","source":"# Use the trained 'trainer' to make predictions on the 'df_test'.\noutputs = trainer.predict(df_test)\n\n# Print the metrics obtained from the prediction outputs.\nprint(outputs.metrics)","metadata":{"_uuid":"fdee54bc-439d-4641-996e-0a16a68b20bb","_cell_guid":"191c9908-f255-47ae-88ec-c93b5080d9d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:21.810609Z","iopub.execute_input":"2024-01-16T17:52:21.810974Z","iopub.status.idle":"2024-01-16T17:52:24.794200Z","shell.execute_reply.started":"2024-01-16T17:52:21.810936Z","shell.execute_reply":"2024-01-16T17:52:24.793318Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"{'test_loss': 0.010881882160902023, 'test_accuracy': 0.9985415653864852, 'test_runtime': 2.9734, 'test_samples_per_second': 691.799, 'test_steps_per_second': 11.098}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Extract the true labels from the model outputs\ny_true = outputs.label_ids\n\n# Predict the labels by selecting the class with the highest probability\ny_pred = outputs.predictions.argmax(1)\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(y_true, y_pred)\n\n# Display accuracy and F1 score\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"_uuid":"65554881-647e-4ef3-a398-7a88edbd0a2c","_cell_guid":"b74a1c44-1893-4aa4-9c04-64029eaa6cad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:24.795684Z","iopub.execute_input":"2024-01-16T17:52:24.795986Z","iopub.status.idle":"2024-01-16T17:52:24.802808Z","shell.execute_reply.started":"2024-01-16T17:52:24.795959Z","shell.execute_reply":"2024-01-16T17:52:24.801903Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Accuracy: 0.9985\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving the model and checking its performance with a sample input","metadata":{"_uuid":"622d7c70-a013-4980-886d-0b6962f59e27","_cell_guid":"ef4b3e09-341a-4e06-94b7-9f7ca61c33f4","trusted":true}},{"cell_type":"code","source":"# Directory where the model output will be saved\n\noutput_dir = \"depressed-tweet-detection-distilbert\"","metadata":{"_uuid":"4dd4e6f0-7713-437d-af85-53254dce893b","_cell_guid":"3693e506-362f-4f53-b431-0480a7d78d53","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:24.808112Z","iopub.execute_input":"2024-01-16T17:52:24.808373Z","iopub.status.idle":"2024-01-16T17:52:24.814382Z","shell.execute_reply.started":"2024-01-16T17:52:24.808351Z","shell.execute_reply":"2024-01-16T17:52:24.813472Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()\ntokenizer.save_vocabulary(save_directory=f\"./{output_dir}\")","metadata":{"_uuid":"8a4c7a88-cd6e-4fb4-8571-0930aac3cd0e","_cell_guid":"5a3f4054-d52e-4708-b9b8-5b1779c0fb4c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:24.815653Z","iopub.execute_input":"2024-01-16T17:52:24.815932Z","iopub.status.idle":"2024-01-16T17:52:25.343843Z","shell.execute_reply.started":"2024-01-16T17:52:24.815907Z","shell.execute_reply":"2024-01-16T17:52:25.342888Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('./depressed-tweet-detection-distilbert/vocab.txt',)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prediction Using saved model","metadata":{"_uuid":"19bc4358-0a6e-4cd6-8dc8-0108b16c339b","_cell_guid":"72436f8c-94cf-4dfe-83da-42a61bca03b7","trusted":true}},{"cell_type":"code","source":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef preprocess_tweet(tweet):\n    # Convert to lowercase\n    tweet = tweet.lower()\n    \n    # Remove URLs\n    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet, flags=re.MULTILINE)\n    \n    # Remove mentions and hashtags\n    tweet = re.sub(r'\\@\\w+|\\#\\w+', '', tweet)\n    \n    # Remove punctuations\n    tweet = tweet.translate(str.maketrans(\"\", \"\", string.punctuation))\n    \n    # Remove numbers\n    tweet = re.sub(r'\\d+', '', tweet)\n    \n    # Tokenize\n    tokens = word_tokenize(tweet)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words]\n    \n    # Join tokens back into a sentence\n    preprocessed_tweet = ' '.join(tokens)\n    \n    return preprocessed_tweet\n\n# Example usage\ntweets = [\n    \"Just had a great workout at the gym! Feeling energized and ready to conquer the day. üí™\"\n\"Rainy days are my favorite. Cozying up with a good book and a hot cup of tea. ‚òîüìñ\"\n\"Excited about the weekend plans! Going hiking with friends. #naturelovers\"\n\"Feeling a bit stressed with work, but looking forward to the weekend. #FridayFeeling\"\n\"Spent the day with loved ones. Grateful for the laughter and good times. ‚ù§Ô∏è\"\n\"Finally got around to watching that movie everyone's been talking about. Such a great story!\"\n\"Productive day at work! Accomplished all my tasks and feeling accomplished. #success\"\n\"Cooked a delicious meal from scratch. Nothing beats homemade comfort food! üç≤\"\n\"Feeling a bit tired, but overall, life is good. Gratitude for the little things.\"\n\"Planning a mini road trip with friends. Can't wait for the adventure! üöó‚ú®\"\n]\n\npreprocessed_tweets = [preprocess_tweet(tweet) for tweet in tweets]\n\nprint(preprocessed_tweets)","metadata":{"_uuid":"0113b10a-6712-4660-a73a-d9b4113799c7","_cell_guid":"a5236d62-2457-48de-ac2e-54c44a74ec76","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:25.345191Z","iopub.execute_input":"2024-01-16T17:52:25.345622Z","iopub.status.idle":"2024-01-16T17:52:25.890869Z","shell.execute_reply.started":"2024-01-16T17:52:25.345566Z","shell.execute_reply":"2024-01-16T17:52:25.889917Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n['great workout gym feeling energized ready conquer day üí™rainy days favorite cozying good book hot cup tea ‚òîüìñexcited weekend plans going hiking friends bit stressed work looking forward weekend day loved ones grateful laughter good times ‚ù§Ô∏èfinally got around watching movie everyones talking great storyproductive day work accomplished tasks feeling accomplished delicious meal scratch nothing beats homemade comfort food üç≤feeling bit tired overall life good gratitude little thingsplanning mini road trip friends cant wait adventure üöó‚ú®']\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir = \"/kaggle/input/depdata\"\n# Pre-trained BERT model to be used\nBERT_MODEL = \"distilbert-base-cased\"\n# Make a classification pipeline and test with the sample input\npipe = pipeline(\"text-classification\", output_dir, tokenizer=BERT_MODEL)\nprediction_results = pipe(preprocessed_tweets, top_k=10)\n# Extract the label and score for each prediction\nfor result in prediction_results:\n    for prediction in result:\n        label = prediction['label']\n        score = prediction['score']\n        print(f\"Label: {label}, Score: {score}\")","metadata":{"_uuid":"2771719c-a1ec-4c38-8d96-67d73adc560d","_cell_guid":"00696e28-3183-4af6-bd1e-478ac50c71e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:25.892197Z","iopub.execute_input":"2024-01-16T17:52:25.892497Z","iopub.status.idle":"2024-01-16T17:52:28.629703Z","shell.execute_reply.started":"2024-01-16T17:52:25.892469Z","shell.execute_reply":"2024-01-16T17:52:28.628733Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Label: Not Depressed, Score: 0.9939814209938049\nLabel: Depressed, Score: 0.006018530111759901\n","output_type":"stream"}]},{"cell_type":"code","source":"# Choose the top prediction based on the highest score\ntop_prediction = max(result, key=lambda x: x['score'])\nprint(f\"\\nTop Prediction: {top_prediction['label']} \\nScore: {top_prediction['score']}\")","metadata":{"_uuid":"422812b0-6712-45c3-8c28-5d21f2cdc01f","_cell_guid":"377a37ea-a9de-46d8-a583-e5475782e612","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:28.630966Z","iopub.execute_input":"2024-01-16T17:52:28.631249Z","iopub.status.idle":"2024-01-16T17:52:28.636675Z","shell.execute_reply.started":"2024-01-16T17:52:28.631223Z","shell.execute_reply":"2024-01-16T17:52:28.635689Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\nTop Prediction: Not Depressed \nScore: 0.9939814209938049\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Code to collect twitter user tweets by using username and detect depression","metadata":{"_uuid":"cad9ce12-a49a-4f00-9ba0-a0a80857890d","_cell_guid":"18c5f7f8-764b-42e4-b416-28e0a4f28b75","trusted":true}},{"cell_type":"code","source":"import tweepy\n# Set up Twitter API credentials\nconsumer_key = \"\"\nconsumer_secret = \"\"\naccess_token = \"\"\naccess_token_secret = \"\"\n\n# Authenticate with Twitter API\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth)","metadata":{"_uuid":"d57b8830-c00f-438c-96b8-c0951430c157","_cell_guid":"97025cf9-916d-4398-b8b3-5e4066166fc9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:28.637970Z","iopub.execute_input":"2024-01-16T17:52:28.638297Z","iopub.status.idle":"2024-01-16T17:52:28.761422Z","shell.execute_reply.started":"2024-01-16T17:52:28.638272Z","shell.execute_reply":"2024-01-16T17:52:28.760517Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# username= input(\"Enter Twitter User Name: \")","metadata":{"_uuid":"19b0e908-8000-44e0-9fc3-ef69953cdf21","_cell_guid":"74a24354-2caf-4342-8ca2-4720369a876c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:28.762594Z","iopub.execute_input":"2024-01-16T17:52:28.762927Z","iopub.status.idle":"2024-01-16T17:52:28.767029Z","shell.execute_reply.started":"2024-01-16T17:52:28.762894Z","shell.execute_reply":"2024-01-16T17:52:28.766187Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# # Function to get tweets from a user\n# def get_tweets(username, count=10):\n#     # Fetch tweets from the user's timeline\n#     tweets = api.user_timeline(screen_name=username, count=count, tweet_mode=\"extended\")\n\n#     # Extract and return the full text of each tweet\n#     return [tweet.full_text for tweet in tweets]","metadata":{"_uuid":"4ca3bdb8-8e50-4597-a0a1-87b73ee2d1df","_cell_guid":"0188825a-59b8-43b7-83e7-7f0309dc54d4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:28.768085Z","iopub.execute_input":"2024-01-16T17:52:28.768342Z","iopub.status.idle":"2024-01-16T17:52:28.778941Z","shell.execute_reply.started":"2024-01-16T17:52:28.768313Z","shell.execute_reply":"2024-01-16T17:52:28.778077Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# user_tweets = get_tweets(username)\n# user_text = \" \".join(user_tweets)","metadata":{"_uuid":"6f418ef9-4c86-4a05-b417-6930dddf26d1","_cell_guid":"f42b0d7a-10f0-4383-b61a-a46cdde2eceb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:28.779993Z","iopub.execute_input":"2024-01-16T17:52:28.780252Z","iopub.status.idle":"2024-01-16T17:52:28.789084Z","shell.execute_reply.started":"2024-01-16T17:52:28.780229Z","shell.execute_reply":"2024-01-16T17:52:28.788255Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\n# # Text preprocessing\n\n# preprocessed_tweets = preprocess_tweet(user_text)\n# # Make a classification pipeline and test with the sample input\n# pipe = pipeline(\"text-classification\", output_dir, tokenizer=BERT_MODEL)\n# prediction_results = pipe(preprocessed_tweets, top_k=10)\n# # Extract the label and score for each prediction\n# for result in prediction_results:\n#     for prediction in result:\n#         label = prediction['label']\n#         score = prediction['score']\n#         print(f\"Label: {label}, Score: {score}\")\n\n# # Choose the top prediction based on the highest score\n# top_prediction = max(result, key=lambda x: x['score'])\n# print(f\"\\nTop Prediction: {top_prediction['label']} \\nScore: {top_prediction['score']}\")","metadata":{"_uuid":"ba18c652-5a0b-4b7c-ab76-eb49ebb1a7b7","_cell_guid":"c06ebe8d-f5cd-4015-80d7-c19b27a7a7f0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-16T17:52:28.790151Z","iopub.execute_input":"2024-01-16T17:52:28.791816Z","iopub.status.idle":"2024-01-16T17:52:28.798671Z","shell.execute_reply.started":"2024-01-16T17:52:28.791790Z","shell.execute_reply":"2024-01-16T17:52:28.797796Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"745c080f-06d5-4388-a1e4-51d1f8d504ba","_cell_guid":"9d8e5413-3154-4b64-8ec7-59ee341c77b9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}